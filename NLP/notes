Introduction to regular expressions

* field of study focused on making sense of language    
                    *using stats and computers 
* basics of nLP
               *topic identifications 
               *text classification
* NLP application include
    * topic identification
    * text classification
* NLP applications include 
    * chatbots
    * translation
    * sentiment analysis

what exactly are regular expression

* strings with a special syntax
* allow us to match patterns in other strings(pattern isa series of letters and symbols which can match to an actual text or words or punctuations)
* applications of regular expression
    * find all web links in document
    * parse email addresses, remove/replace unwanted characters                     * regex lib
    * import re 
    * matching substring by re.match method which matches pattern with string it takes pattern as first argument and string as second argument and returns match object
re.match(‘abc’,’abcdef’)
    * can use special patterns such as ‘\w+’,here we can see match word representations matching the first word it found ‘hi’ match=hi
word_regex = ‘\w’
re.match(word_regex,’hi there!’)

* common regex patterns 

\w+  matches words e.g ‘magic’
\d     matches digits e.g 9
\s     matches spaces  egg ‘’
.*      is wildcard character  will match any letter and symbols
+or*  allows things to be greedy grabbing repeats of single letters or whole patterns e.g for matching the full word  rather then one charter we need to add the + symbol after the \w
    * \capital negates them
\S  matches not space that is ‘not_spaces’
    * creating a group of character we want 
[a-z] lowercase group ‘abcdefg’


* Python re module
    * re module
    * split—split a string on regex
    * findall —find all patterns in string 
    * search—search for a pattern
    * match—match an entire string or substring based on pattern 

syntax for regex lib is away to pass pattern first and then string second  which may return an iterator , string or match object 
Practicing regular expressions: re.split() and re.findall()
Now you'll get a chance to write some regular expressions to match digits, strings and non-alphanumeric characters. Take a look at my_string first by printing it in the IPython Shell, to determine how you might best match the different steps.
Note: It's important to prefix your regex patterns with r to ensure that your patterns are interpreted in the way you want them to. Else, you may encounter problems to do with escape sequences in strings. For example, "\n" in Python is used to indicate a new line, but if you use the r prefix, it will be interpreted as the raw string "\n" - that is, the character "\" followed by the character "n" - and not as a new line.
re.split(‘\s+’,’split on space.’)
[‘split’,’on’,’space.’]

this can be used for tokenisation

In [1]: my_string = "Let's write RegEx!"

In [2]: re.findall(PATTERN, my_string)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
    re.findall(PATTERN, my_string)
NameError: name 'PATTERN' is not defined

In [3]: re.findall(r"\s+"
... 
... , my_string)
... 
Out[3]: [' ', ' ']

In [4]: re.findall(r"\s+", my_string)
Out[4]: [' ', ' ']

In [5]: re.findall(r"\w+", my_string)
Out[5]: ['Let', 's', 'write', 'RegEx']

In [6]: re.findall(r"[a-z]+", my_string)
Out[6]: ['et', 's', 'write', 'eg', 'x']

In [7]: re.findall(r"\w", my_string)
Out[7]: ['L', 'e', 't', 's', 'w', 'r', 'i', 't', 'e', 'R', 'e', 'g', 'E', 'x']

* What is Tokenisation ?

Turning a string or document into tokens(smaller chunks)
one step in preparing a text for NLP
many different theories and rules
you can create your rules using regular expressions

some examples
breaking out words or sentences 
separating punctuation
separating all hashtags in a tweets

tokenisation library nltk library

short e.g of yuse of tokenise methods

from nltk.tokenize import word_tokenize
word_tokenize(“Hi there”)

why tokenize?

    * easier to map part of speech
    * matching common words
    * removing unwanted tokens
    * e.g i don’t like sams shoes
    * “I”,”do”,”n’t”,”like”,”sam”,”s”,”shoes”,”.”
* other nltk tokenisers
    * sent_tokenize: tokenize a document into sentences
    * regexp_tokenize:tokenize a string or document based on regular expression
    * tweettokenizer:special class just for tweet tokenisation , allowing you to separate hashtags , mentions and lots of exclamations
more regex practice 

difference between re.search() and r.match()
import re 
re.match(‘abc’,’abcde’)
re.search(‘abc’,’abcde’)
re.match(‘cd’,’abcde’)
re.search(‘cd’,’abcde’)


# Import necessary modules
Monty Python's Holy Grail, which has been pre-loaded as scene_one
from nltk.tokenize import sent_tokenize
from nltk.tokenize import word_tokenize

# Split scene_one into sentences: sentences
sentences = sent_tokenize(scene_one)

# Use word_tokenize to tokenize the fourth sentence: tokenized_sent
tokenized_sent = word_tokenize(sentences[3])

# Make a set of unique tokens in the entire scene: unique_tokens
unique_tokens = set(word_tokenize(scene_one))

# Print the unique tokens result
print(unique_tokens)

# Search for the first occurrence of "coconuts" in scene_one: match
match = re.search("coconuts", scene_one)

# Print the start and end indexes of match
print(match.start(), match.end())

# Write a regular expression to search for anything in square brackets: pattern1
pattern1 = r"\[.*\]"

# Use re.search to find the first text in square brackets
print(re.search(pattern1, scene_one))

# Find the script notation at the beginning of the fourth sentence and print it
pattern2 = r"[\w\s]+:"
print(re.match(pattern2,sentences[3]))


Advanced tokenisation with NLTK and regex 

regex groups using or”|”

    * OR is represented using | — pipe character
    * you can define a *group using ()—groups can be either patterns or character u want to match.
    * you can define explicit character *ranges using []
we want to find all words and digits 
import re 
match_digits_and_words = (‘(\d+|\w+)’) — we define the pattern using or symbol and make them greedy so that they catch the full word or digit then we can call re.findall(match_digits_and_words,’he has 11 cats’)

* regex ranges and groups 
other patterns 
[A-Za-Z]+    ——matches upper and lowercases english alphabet —e.g[ABCDEFghiijk’
[0-9] ————-number from 0 to 9 ——e.g 9
[A-Za-z\-\.]+—matches super and lowercases english alphabets,-and.—e.g ‘My-Webite.com’
(a-z)——matches a,-and z  ——e.g ‘a-z’
(\s+l,)——spaces or a comma——e.g ‘,’

import re
my_str = ‘match lowercase spaces mums like 12,but no commas’

re.match(‘[a-z0-9]+’,my_str)
are,SRE_Match object;
span=(0,42),match=‘match lowercase mums like 12’>

* Non-ascii tokenization
In this exercise, you'll practice advanced tokenization by tokenizing some non-ascii based text. You'll be using German with emoji!
Here, you have access to a string called german_text, which has been printed for you in the Shell. Notice the emoji and the German characters!
The following modules have been pre-imported from nltk.tokenize: regexp_tokenize and word_tokenize.
Unicode ranges for emoji are:
('\U0001F300'-'\U0001F5FF'), ('\U0001F600-\U0001F64F'), ('\U0001F680-\U0001F6FF'), and ('\u2600'-\u26FF-\u2700-\u27BF')
# Tokenize and print all words in german_text
all_words = word_tokenize(german_text)
print(all_words)

# Tokenize and print only capital words
capital_words = r"[A-ZÜ]\w+"
print(regexp_tokenize(german_text, capital_words))

# Tokenize and print only emoji
emoji = "['\U0001F300-\U0001F5FF'|'\U0001F600-\U0001F64F'|'\U0001F680-\U0001F6FF'|'\u2600-\u26FF\u2700-\u27BF']"
print(regexp_tokenize(german_text, emoji))

* charting NLP data extortion with plotting 
* charting NLP data extraction with plotting 
from matplotlib import pyplot as plt
from nltk.tokenize import word_tokenize
words = word_tokenize(“This is a pretty cool tool”)
word_lengths= [len(w) for w in words]

plt.hist(word_lengths)
put.show()

* charting practice 
* 
# Split the script into lines: lines
lines = holy_grail.split('\n')

# Replace all script lines for speaker
pattern = "[A-Z]{2,}(\s)?(#\d)?([A-Z]{2,})?:"
lines = [re.sub(pattern, '', l) for l in lines]

# Tokenize each line: tokenized_lines
tokenized_lines = [regexp_tokenize(s,"\w+") for s in lines]

# Make a frequency list of lengths: line_num_words
line_num_words = [len(t_line) for t_line in tokenized_lines]

# Plot a histogram of the line lengths
plt.hist(line_num_words)

# Show the plot
plt.show()

regex 
https://www.dataquest.io/blog/regular-expressions-data-scientists/
