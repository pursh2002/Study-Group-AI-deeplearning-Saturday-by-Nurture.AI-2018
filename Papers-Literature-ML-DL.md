
* https://www.arxiv-vanity.com/papers/1803.06131/

* https://github.com/GauravBh1010tt/DeepLearn

* https://medium.com/nurture-ai/https-medium-com-nurture-ai-a-collaborative-platform-for-getting-more-out-of-ai-research-b9888df20b6e

* https://towardsdatascience.com/deep-learning-with-apache-spark-part-1-6d397c16abd

reading research paper :
https://www.youtube.com/watch?v=SHTOI0KtZnU--siraj reading reseach paper
https://medium.com/ai-saturdays/how-to-read-academic-papers-without-freaking-out-3f7ef43a070f

{https://www.reddit.com/r/MachineLearning/
https://www.reddit.com/r/MachineLearning/
https://research.google.com}

[https://github.com/pursh2002/Papers-Literature-ML-DL
[https://peerj.com/articles/453.pdf
[http://www.marekrei.com/blog/paper-summaries/?lipi=urn%3Ali%3Apage%3Ad_flagship3_feed%3BVGtsKX8YShGi8i5T5gy%2BjA%3D%3D

These are the 5 papers that we will be covering in the AMA on 5am-10am Friday 26th of January 2018 Coordinated Universal Time (UTC).

[https://nurture.ai/p/41166f98-40d8-4af8-bc89-3b2146c9a456---A Multi-Agent Reinforcement Learning Model of Common-Pool Resource Appropriation
[https://nurture.ai/p/b1007b2c-3be4-46e6-9d46-094461f51614---Data Augmentation by Pairing Samples for Image Classification
[https://nurture.ai/p/eb6fd99b-a0cc-4aec-ab42-1a19465198f4---Generating Adversarial Examples with Adversarial Networks
[https://nurture.ai/p/ff91053e-7115-4dce-b346-d39e74583703---Less is More: Culling the Training Set to Improve Robustness of Deep Neural Networks
[https://nurture.ai/p/a02d8113-36a4-4783-954f-a48d37098b20eCommerce---GAN: A Generative Adversarial Network for E-commerce15

[https://medium.com/applied-data-science/how-to-build-your-own-alphazero-ai-using-python-and-keras-7f664945c188
[https://arxiv.org/pdf/1502.05698.pdf

[https://medium.com/applied-data-science/how-to-build-your-own-alphazero-ai-using-python-and-keras-7f664945c188

[Nurture.AI Research Fellows are hosting yet another round of AMA for this week's top AI research papers! https://lnkd.in/fisPtT6

Here are the papers they are covering this week, with annotations in comments by our research fellows:
PointCNN: https://lnkd.in/fzN2Mdn
Letâ€™s Dance: Learning From Online Dance Videos: https://lnkd.in/fTyskYH
PRNN: Recurrent Neural Network with Persistent Memory: https://lnkd.in/fCuCnq5
HappyDB: A Corpus of 100,000 Crowdsourced Happy Moments: https://lnkd.in/fjsHZsJ
Personalizing Dialogue Agents: I have a dog, do you have pets too?: https://lnkd.in/fmyPhpK


[https://towardsdatascience.com/how-do-we-train-neural-networks-edd985562b73

https://goo.gl/nJr5KH deep learnig

## These are the 5 papers that we will be covering in the AMA on 5am-10am Friday 26th of January 2018 Coordinated Universal Time (UTC).

https://nurture.ai/p/41166f98-40d8-4af8-bc89-3b2146c9a456-- A Multi-Agent Reinforcement Learning Model of Common-Pool Resource Appropriation23
https://nurture.ai/p/b1007b2c-3be4-46e6-9d46-094461f51614-- Data Augmentation by Pairing Samples for Image Classification12
https://nurture.ai/p/eb6fd99b-a0cc-4aec-ab42-1a19465198f4-- Generating Adversarial Examples with Adversarial Networks9
https://nurture.ai/p/ff91053e-7115-4dce-b346-d39e74583703--Less is More: Culling the Training Set to Improve Robustness of Deep Neural Networks6
https://nurture.ai/p/a02d8113-36a4-4783-954f-a48d37098b20-- eCommerceGAN: A Generative Adversarial Network for E-commerce21


https://github.com/pursh2002/DL-2

https://www.arxiv-vanity.com/papers/1802.07740/?utm_content=buffera6f07&utm_medium=social&utm_source=linkedin.com&utm_campaign=buffer

https://arxiv.org/pdf/1611.01491.pdf
https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf
https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf


ML 
Statistics, Probability and Calculus.
http://www2.math.uu.se/~thulin/mm/breiman.pdf -- good start to understand the views of statistical and computer science research community about Statistical Learning

http://vita.had.co.nz/papers/tidy-data.pdf -- data preparation and preprocessing

https://www.analyticsvidhya.com/blog/2015/08/comprehensive-guide-regression/

https://ti.arc.nasa.gov/m/profile/dhw/papers/78.pdf

Data Representation: Where you transform data to another space (usually vector space).
https://arxiv.org/pdf/1301.3781.pdf --- words in vector space
https://cs.stanford.edu/~quocle/paragraph_vector.pdf -- A document to vector

Learning:
http://ruder.io/optimizing-gradient-descent/
http://www.maths.ed.ac.uk/~prichtar/papers/Papamakarios.pdf
https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/

Evaluation: Empirical evaluation of a model is very important, to understand the model and infer from it.

https://www.analyticsvidhya.com/blog/2016/02/7-important-model-evaluation-error-metrics/
http://pages.cs.wisc.edu/~dpage/cs760/evaluating.pdf
http://www.damienfrancois.be/blog/files/modelperfcheatsheet.pdf%20(http://www.damienfrancois.be/blog/files/modelperfcheatsheet.pdf

Paper:
https://lnkd.in/dPC2h-R
Code:
https://lnkd.in/dJzCHMi


https://arxiv.org/abs/1603.08155
http://research.nvidia.com/publication/2017-10_Progressive-Growing-of
https://news.ycombinator.com/item?id=15572790
https://github.com/Avhirup/Progressive-Growing-Of-GANs-Pytorch-

https://arxiv.org/abs/1506.01186

http://forums.fast.ai/t/wiki-lesson-2/9399

https://adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html

https://www.jeremyjordan.me/neural-networks-training/

GAN : https://lnkd.in/ddCmS9j?lipi=urn%3Ali%3Apage%3Ad_flagship3_feed%3BO4efC7IAT7irll63pQZKoA%3D%3D

Alberto:
Are CNNs better than RNNs (including LSTM and GRU) for sequence modeling? 

This is the strong claim of an important new paper from Carnegie Mellon and Intel: An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling, by Shaojie Bai, J. Zico Kolter, Vladlen Koltun.

Authors carried out an extensive comparison of simple CNNs vs LSTM and GRU on a large number of tasks. According to the authors, their general CNNs:

"convincingly outperform baseline recurrent architectures across a broad range of sequence modeling tasks."

"exhibit substantially longer memory, and are thus more suitable for domains where a long history is required."

"Until recently, before the introduction of architectural elements such as dilated convolutions and residual connections, convolutional architectures were indeed weaker. Our results indicate that with these elements, a simple convolutional architecture is more effective across diverse sequence modeling tasks than recurrent architectures such as LSTMs.

"We conclude that convolutional networks should be regarded as a natural starting point and a powerful toolkit for sequence modeling."

Arxiv: https://lnkd.in/f2kr6dE
Github: https://lnkd.in/fY5TTDd

"we use ablation analyses to measure the reliance of trained networks on single directions. We define a single direction in activation space as the activation of a single unit or feature map or some linear combination of units in response to some input. We find that networks which memorize the training set are substantially more dependent on single directions than those which do not, and that this difference is preserved even across sets of networks with identical topology trained on identical data, but with different generalization performance. Moreover, we found that as networks begin to overfit, they become more reliant on single directions, suggesting that this metric could be used as a signal for early stopping."

Credit: Ari S. Morcos, David G.T. Barrett, Neil C. Rabinowitz, & Matthew Botvinick

paper: https://lnkd.in/fhQ9v42
blog: https://lnkd.in/f7Cprsq

https://towardsdatascience.com/a-weird-introduction-to-deep-learning-7828803693b0

The following model, GBFS (Gradient Boosted Feature Selection) in [1], a modification of Gradient Boosted Trees to do feature selection in a binary classification task. The Matlab code for GBFS is available in [2], so learning enthusiasts may want to check it out.

A previous related post on a hybrid of XGboost & Wavelet here https://lnkd.in/f3m5ERj is relevant as well.

Overview:
The 4 conditions that a feature selection algorithm should ideally satisfy:
    a) reliably extract relevant features; 
    b) be able to identify non-linear feature interactions; 
    c) scale linearly with the number of features and dimensions; 
    d) allow the incorporation of known sparsity structure; 
The proposed GBFS satisfies all 4 of these requirements. Test on several real world data-sets show that it matches or outperforms other state of the art feature selection algorithms. Yet it scales to larger data set sizes and naturally allows for domain-specific side information.

The README text file has the instructions on mex compiling the C files and also how to run the 'example.m' demo file in the GBFS package.

[1] " Gradient Boosted Feature Selection " (PDF)
https://lnkd.in/fdHJAqK

[2] " GBFS Matlab Code " (Zip)
https://lnkd.in/fbYMiex
https://www.datasciencecentral.com/profiles/blogs/a-curated-list-of-resources-dedicated-to-bayesian-deep-learning

https://www.kdnuggets.com/2017/04/top-20-papers-machine-learning.html

https://www.kdnuggets.com/2015/05/dark-knowledge-neural-network.html

From text-to-image. Image generation from text with Generative Adversarial Networks. 

"Deep Attentional Multimodal Similarity Network"

Paper: https://lnkd.in/dK95Wyy
Code: https://lnkd.in/dNC7wed
